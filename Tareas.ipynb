{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "13ANsMYRJET3BCF9jqa6wtAWlVnG5j6JI",
      "authorship_tag": "ABX9TyN7E59g1R+F5jzu5tJxddJP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mikeroguez/experimento/blob/main/Tareas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Codigo para normalizar**"
      ],
      "metadata": {
        "id": "4Wb4l-gvhvds"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cM7HvPG6jwnp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f651faf-e9e5-40c8-cf9b-967f156e03ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset procesado guardado en: /content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_normalizado.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import html\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "def get_semester_bounds(commitment_date: pd.Timestamp) -> Tuple[pd.Timestamp, pd.Timestamp]:\n",
        "    \"\"\"Determina los límites del semestre basado en la fecha de compromiso.\"\"\"\n",
        "    if pd.isnull(commitment_date):\n",
        "        return pd.NaT, pd.NaT\n",
        "\n",
        "    year, month = commitment_date.year, commitment_date.month\n",
        "\n",
        "    if 2 <= month <= 7:\n",
        "        return pd.Timestamp(year, 2, 1), pd.Timestamp(year, 7, 31)\n",
        "    elif month == 1:\n",
        "        return pd.Timestamp(year - 1, 8, 1), pd.Timestamp(year, 1, 31)\n",
        "    else:\n",
        "        return pd.Timestamp(year, 8, 1), pd.Timestamp(year + 1, 1, 31)\n",
        "\n",
        "def process_data(input_path: str, output_path: str) -> pd.DataFrame:\n",
        "    \"\"\"Procesa el dataset de tareas académicas y guarda el resultado.\"\"\"\n",
        "    FINAL_COLUMNS = [\n",
        "        'curso', 'groupKey', 'email', 'id_actividad', 'actividad',\n",
        "        'fecha_compromiso', 'fecha_de_entrega', 'calificacion',\n",
        "        'calificacion_ajustada', 'is_atypical',\n",
        "        'calificacion_normalizada', 'calificacion_estandarizada',\n",
        "        'dias_anticipacion', 'fuera_de_rango', 'fue_entregada', 'fue_evaluada', 'tipo_entrega'\n",
        "    ]\n",
        "\n",
        "    # Cargar y preparar los datos\n",
        "    df = pd.read_csv(input_path)\n",
        "    df.rename(columns={\n",
        "        'Fecha_compromiso': 'fecha_compromiso',\n",
        "        'Fecha_de_entrega': 'fecha_de_entrega',\n",
        "        'Calificacion': 'calificacion',\n",
        "        'Actividad': 'actividad'\n",
        "    }, inplace=True)\n",
        "\n",
        "    # Parsear fechas\n",
        "    df['fecha_compromiso'] = pd.to_datetime(df['fecha_compromiso'], errors='coerce')\n",
        "    df['fecha_de_entrega'] = pd.to_datetime(df['fecha_de_entrega'], errors='coerce')\n",
        "\n",
        "    # Limpiar y normalizar la columna 'actividad'\n",
        "    df['actividad'] = (\n",
        "        df['actividad']\n",
        "        .fillna(\"actividad desconocida\")\n",
        "        .apply(lambda x: html.unescape(str(x)))\n",
        "        .str.replace(r'[^\\x00-\\x7F]+', '', regex=True)\n",
        "    )\n",
        "\n",
        "    # **Ajuste de valores atípicos con verificación**\n",
        "    actividad_stats = df.groupby('id_actividad')['calificacion'].agg(\n",
        "        p99=lambda x: np.percentile(x.dropna(), 99) if x.dropna().size > 0 else np.nan\n",
        "    ).reset_index()\n",
        "\n",
        "    df = df.merge(actividad_stats, on='id_actividad', how='left')\n",
        "\n",
        "    # Crear columna `is_atypical` y ajustar valores atípicos\n",
        "    df['is_atypical'] = (df['calificacion'] > df['p99']).astype(int)\n",
        "    df['calificacion_ajustada'] = np.where(\n",
        "        df['calificacion'] > df['p99'], df['p99'], df['calificacion']\n",
        "    )\n",
        "\n",
        "    # **Normalización basada en el máximo de la actividad**\n",
        "    max_scores = df.groupby('id_actividad')['calificacion_ajustada'].max().reset_index()\n",
        "    max_scores.rename(columns={'calificacion_ajustada': 'max_calificacion'}, inplace=True)\n",
        "    df = df.merge(max_scores, on='id_actividad', how='left')\n",
        "\n",
        "    df['calificacion_normalizada'] = np.where(\n",
        "        df['calificacion_ajustada'].notna(),\n",
        "        (df['calificacion_ajustada'] / df['max_calificacion'] * 10).round(2),\n",
        "        np.nan  # Si no hay calificación, normalización también debe ser NaN\n",
        "    )\n",
        "\n",
        "    # **Estandarización (Z-score con corrección)**\n",
        "    actividad_stats = df.groupby('id_actividad')['calificacion_ajustada'].agg(mean='mean', std='std', count='count')\n",
        "    df = df.merge(actividad_stats, left_on='id_actividad', right_index=True, how='left')\n",
        "\n",
        "    df['calificacion_estandarizada'] = np.where(\n",
        "        df['calificacion_ajustada'].isna(),  # Si no hay calificación\n",
        "        np.nan,  # Mantener NaN\n",
        "        np.where(\n",
        "            df['count'] == 1,  # Si solo hay una calificación en la actividad\n",
        "            0,  # Z-score = 0 (único dato disponible)\n",
        "            ((df['calificacion_ajustada'] - df['mean']) / np.where(df['std'] == 0, 1, df['std'])).round(2)  # Evitar división por 0\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # **Cálculo de días de anticipación**\n",
        "    df['dias_anticipacion'] = (df['fecha_compromiso'] - df['fecha_de_entrega']).dt.days\n",
        "\n",
        "    # **Cálculo de `fuera_de_rango`**\n",
        "    df[['semester_start', 'semester_end']] = df['fecha_compromiso'].apply(lambda x: pd.Series(get_semester_bounds(x)))\n",
        "\n",
        "    df['fuera_de_rango'] = df.apply(\n",
        "        lambda x: (\n",
        "            pd.notna(x['fecha_de_entrega']) and\n",
        "            pd.notna(x['semester_start']) and\n",
        "            pd.notna(x['semester_end']) and\n",
        "            (x['fecha_de_entrega'] < x['semester_start'] or x['fecha_de_entrega'] > x['semester_end'])\n",
        "        ),\n",
        "        axis=1\n",
        "    ).astype(int)  # Convertir a 0/1\n",
        "\n",
        "    # **Indicadores de entrega y evaluación**\n",
        "    df['fue_entregada'] = df['fecha_de_entrega'].notna().astype(int)\n",
        "    df['fue_evaluada'] = df['calificacion'].notna().astype(int)\n",
        "\n",
        "    # **Clasificación del tipo de entrega**\n",
        "    df['tipo_entrega'] = np.select(\n",
        "        [\n",
        "            df['fecha_de_entrega'].isna(),\n",
        "            df['dias_anticipacion'] > 7,\n",
        "            df['dias_anticipacion'].between(0, 7),\n",
        "            df['dias_anticipacion'] < 0\n",
        "        ],\n",
        "        ['faltante', 'temprana', 'puntual', 'tardía'],\n",
        "        default='desconocida'\n",
        "    )\n",
        "\n",
        "    return df[FINAL_COLUMNS].copy()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    INPUT_PATH = \"/content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas.csv\"\n",
        "    OUTPUT_PATH = \"/content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_normalizado.csv\"\n",
        "\n",
        "    processed_df = process_data(INPUT_PATH, OUTPUT_PATH)\n",
        "    processed_df.to_csv(OUTPUT_PATH, index=False)\n",
        "    print(f\"Dataset procesado guardado en: {OUTPUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Código para consolidar**"
      ],
      "metadata": {
        "id": "RmZfYPEOh1Z_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargar el dataset desde Google Drive\n",
        "file_path = \"/content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_normalizado.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Función para consolidar los datos con cálculos consistentes\n",
        "def consolidate_data_fixed(data):\n",
        "    # Agrupar y calcular métricas principales\n",
        "    consolidated = data.groupby(['curso', 'email', 'groupKey']).agg(\n",
        "        total_assignments=('id_actividad', 'count'),\n",
        "        avg_assignment_normalized_score=('calificacion_normalizada', 'mean'),\n",
        "        avg_assignment_estandard_score=('calificacion_estandarizada', 'mean'),\n",
        "        max_assignment_normalized_score=('calificacion_normalizada', 'max'),\n",
        "        min_assignment_normalized_score=('calificacion_normalizada', 'min'),\n",
        "        max_assignment_estandard_score=('calificacion_estandarizada', 'max'),\n",
        "        min_assignment_estandard_score=('calificacion_estandarizada', 'min'),\n",
        "        score_variability=('calificacion_normalizada', 'std'),\n",
        "        total_graded_assignments=('fue_evaluada', 'sum'),\n",
        "        total_submitted_assignments=('fue_entregada', 'sum'),\n",
        "        total_missing_assignments=('fue_entregada', lambda x: (x == 0).sum()),\n",
        "        total_ungraded_assignments=('fue_evaluada', lambda x: (x == 0).sum()),\n",
        "        total_out_of_range_assignments=('fuera_de_rango', 'sum'),\n",
        "        total_late_assignments=('tipo_entrega', lambda x: (x == 'tardía').sum()),\n",
        "        avg_submission_delay_days=(\n",
        "            'dias_anticipacion',\n",
        "            lambda x: x[data.loc[x.index, 'fue_entregada'] == 1].mean() if not x.empty else 0\n",
        "        )\n",
        "    ).reset_index()\n",
        "\n",
        "    # Calcular tasas y métricas derivadas\n",
        "    consolidated['assignment_submission_rate'] = (\n",
        "        consolidated['total_submitted_assignments'] / consolidated['total_assignments']\n",
        "    )\n",
        "    consolidated['assignment_procrastination_rate'] = (\n",
        "        consolidated['total_late_assignments'] / consolidated['total_submitted_assignments']\n",
        "    )\n",
        "\n",
        "    # Calcular rendimiento general ponderado\n",
        "    w1, w2 = 0.5, 0.5\n",
        "    consolidated['overall_assignment_performance'] = (\n",
        "        (consolidated['avg_assignment_normalized_score'] * w1) +\n",
        "        (consolidated['assignment_submission_rate'] * 10 * w2)\n",
        "    )\n",
        "\n",
        "    # Clasificar rendimiento general\n",
        "    def classify_general_performance(overall_score):\n",
        "        if overall_score >= 9.0:\n",
        "            return 'Excelente'\n",
        "        elif overall_score >= 8.0:\n",
        "            return 'Bien'\n",
        "        elif overall_score >= 6.0:\n",
        "            return 'Suficiente'\n",
        "        else:\n",
        "            return 'Insuficiente'\n",
        "\n",
        "    consolidated['overall_assignment_performance_classification'] = (\n",
        "        consolidated['overall_assignment_performance'].apply(classify_general_performance)\n",
        "    )\n",
        "\n",
        "    # Revisar si todos los trabajos del grupo no fueron calificados\n",
        "    group_status = consolidated.groupby(['curso', 'groupKey'])['total_graded_assignments'].sum().reset_index()\n",
        "    group_status['delete'] = group_status['total_graded_assignments'].apply(lambda x: 1 if x == 0 else 0)\n",
        "\n",
        "    # Unir el estado del grupo al DataFrame consolidado\n",
        "    consolidated = pd.merge(\n",
        "        consolidated, group_status[['curso', 'groupKey', 'delete']],\n",
        "        on=['curso', 'groupKey'], how='left'\n",
        "    )\n",
        "\n",
        "    # Determinar si se debe marcar course_retake_probability como 1\n",
        "    course_status = consolidated.groupby('curso').apply(\n",
        "        lambda x: 1 if (x['total_submitted_assignments'] > 0).any() and\n",
        "                        all(x.loc[x['total_submitted_assignments'] > 0, 'total_out_of_range_assignments'] > 0) and\n",
        "                        abs(x['avg_submission_delay_days'].mean()) > 30\n",
        "                  else 0\n",
        "    ).reset_index(name='course_retake_probability')\n",
        "\n",
        "    # Unir el estado del curso al DataFrame consolidado\n",
        "    consolidated = pd.merge(consolidated, course_status, on='curso', how='left')\n",
        "\n",
        "    return consolidated\n",
        "\n",
        "# Aplicar la función para consolidar los datos\n",
        "corrected_consolidated_data = consolidate_data_fixed(data)\n",
        "\n",
        "# Guardar los datos consolidados corregidos\n",
        "output_path_corrected = \"/content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_consolidado.csv\"\n",
        "corrected_consolidated_data.to_csv(output_path_corrected, index=False)\n",
        "\n",
        "print(f\"Archivo consolidado guardado en: {output_path_corrected}\")\n"
      ],
      "metadata": {
        "id": "Wio0XzPJS3JZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8d69484-cf81-4c27-a5cc-cf7c50aeba87"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-c81e60e9ceae>:72: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  course_status = consolidated.groupby('curso').apply(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo consolidado guardado en: /content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_consolidado.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Código para limpiar**"
      ],
      "metadata": {
        "id": "zrNj_50Qh6ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ruta del archivo de entrada y salida\n",
        "input_file = \"/content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_consolidado.csv\"\n",
        "output_file = \"/content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_consolidado_cleaned.csv\"\n",
        "\n",
        "# Cargar el archivo consolidado\n",
        "data = pd.read_csv(input_file)\n",
        "\n",
        "# Filtrar registros donde delete = 1 y eliminar la columna 'delete'\n",
        "def filter_data(data):\n",
        "    # Filtrar registros donde delete != 1\n",
        "    filtered_data = data[data['delete'] != 1].copy()\n",
        "\n",
        "    # Eliminar la columna 'delete'\n",
        "    filtered_data.drop(columns=['delete'], inplace=True)\n",
        "\n",
        "    return filtered_data\n",
        "\n",
        "# Manejar valores nulos e imputar\n",
        "def handle_missing_values(data):\n",
        "    # Imputar valores nulos en las columnas relevantes\n",
        "    fill_zero_columns = [\n",
        "        'avg_assignment_normalized_score', 'avg_assignment_estandard_score',\n",
        "        'max_assignment_normalized_score', 'min_assignment_normalized_score',\n",
        "        'max_assignment_estandard_score', 'min_assignment_estandard_score',\n",
        "        'score_variability', 'avg_submission_delay_days',\n",
        "        'assignment_procrastination_rate', 'overall_assignment_performance'\n",
        "    ]\n",
        "    data[fill_zero_columns] = data[fill_zero_columns].fillna(0)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Aplicar el pipeline\n",
        "data_filtered = filter_data(data)\n",
        "data_cleaned = handle_missing_values(data_filtered)\n",
        "\n",
        "# Guardar el archivo limpio\n",
        "data_cleaned.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Archivo limpio guardado en: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z-MTIv_eKSZ",
        "outputId": "5ecd7954-5bc9-4027-f4c8-57e83db8f801"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo limpio guardado en: /content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_consolidado_cleaned.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Elimina cursos posiblemente reutilizados**"
      ],
      "metadata": {
        "id": "yANn2c8k3cwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Definir archivos de entrada y salida\n",
        "input_file = \"/content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_consolidado_cleaned.csv\"\n",
        "output_file = \"/content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_consolidado_cleaned_sin_cursos_reutilizados.csv\"\n",
        "\n",
        "# Cargar el dataset\n",
        "data = pd.read_csv(input_file)\n",
        "\n",
        "# Filtrar registros donde course_retake_probability != 1\n",
        "data_filtered = data[data['course_retake_probability'] != 1]\n",
        "\n",
        "# Eliminar la columna course_retake_probability\n",
        "data_filtered = data_filtered.drop(columns=['course_retake_probability'])\n",
        "\n",
        "# Guardar el resultado en un nuevo archivo\n",
        "data_filtered.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Archivo guardado en: {output_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0SwLYoC3h2G",
        "outputId": "278f3335-f1f6-4a03-8854-f8baad84f138"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo guardado en: /content/drive/MyDrive/Doctorado/Experimentos/Datos/DSv4.0/1. Para tratar/tareas/tareas_consolidado_cleaned_sin_cursos_reutilizados.csv\n"
          ]
        }
      ]
    }
  ]
}